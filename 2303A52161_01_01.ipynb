{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.⁠ ⁠(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
    "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
    "with the outcomes of libraries\n",
    "YActual YP red\n",
    "20 20.5\n",
    "30 30.3\n",
    "40 40.2\n",
    "50 50.6\n",
    "60 60.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.24600000000000147\n",
      "Root Mean Squared Error: 0.49598387070549127\n",
      "R-squared : 0.99877\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test=[20,30,40,50,60]\n",
    "y_pred=[20.5,30.3,40.2,50.6,60.7]\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared : {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.24600000000000147\n",
      "Root Mean Squared Error: 0.49598387070549127\n",
      "R-squared : 0.99877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared : {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.⁠ ⁠(1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
    "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
    "results with outcome of libraries\n",
    "YActual YP red\n",
    "0 0 1 1 2 0\n",
    "0 0 1 0 2 0\n",
    "0 1 1 2 2 1\n",
    "0 2 1 0 2 2\n",
    "0 2 1 2 2 2\n",
    "Tabela 2: YActual Vs. YP red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 1 2]\n",
      " [3 0 2]\n",
      " [2 1 2]]\n",
      "Accuracy: 0.26666666666666666\n",
      "Precision: 0.20634920634920637\n",
      "Recall: 0.26666666666666666\n",
      "F1 Score: 0.23232323232323235\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "y_actual = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])\n",
    "y_pred = np.array([0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 2, 2, 2, 2])\n",
    "conf_matrix = confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_actual, y_pred)\n",
    "prec = precision_score(y_actual, y_pred, average='macro')\n",
    "recall = recall_score(y_actual, y_pred, average='macro')\n",
    "f1 = f1_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (lib):\n",
      "[[2 1 2]\n",
      " [3 0 2]\n",
      " [2 1 2]]\n",
      "Accuracy (lib): 0.26666666666666666\n",
      "Precision (lib): 0.20634920634920637\n",
      "Recall (lib): 0.26666666666666666\n",
      "F1 Score (lib): 0.23232323232323235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "conf_matrix_lib = confusion_matrix(y_actual, y_pred)\n",
    "accuracy_lib = accuracy_score(y_actual, y_pred)\n",
    "precision_lib = precision_score(y_actual, y_pred, average='macro')\n",
    "recall_lib = recall_score(y_actual, y_pred, average='macro')\n",
    "f1_lib = f1_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "print(\"Confusion Matrix (lib):\")\n",
    "print(conf_matrix_lib)\n",
    "print(f\"Accuracy (lib): {accuracy_lib}\")\n",
    "print(f\"Precision (lib): {precision_lib}\")\n",
    "print(f\"Recall (lib): {recall_lib}\")\n",
    "print(f\"F1 Score (lib): {f1_lib}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
